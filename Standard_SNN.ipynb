{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIbccjhvAv7rQtnDbMvkew",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrewsiyoon/spiking-seRNN/blob/main/Standard_SNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9RLdQCmw2sd"
      },
      "outputs": [],
      "source": [
        "!pip install snntorch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import snntorch as snn\n",
        "from snntorch import spikeplot as splt\n",
        "from snntorch import spikegen\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import itertools"
      ],
      "metadata": {
        "id": "UjY3_cxsUtY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Leaky neuron model, overriding the backward pass with a custom function (surrogate gradient descent)\n",
        "class LeakySurrogate(nn.Module):\n",
        "  def __init__(self, beta, threshold=1.0):\n",
        "      super(LeakySurrogate, self).__init__()\n",
        "\n",
        "      # initialize decay rate beta and threshold\n",
        "      self.beta = beta\n",
        "      self.threshold = threshold\n",
        "      self.spike_op = self.SpikeOperator.apply\n",
        "  \n",
        "  # the forward function is called each time we call Leaky\n",
        "  def forward(self, input_, mem):\n",
        "    spk = self.spike_op((mem-self.threshold))  # call the Heaviside function\n",
        "    reset = (spk * self.threshold).detach() # removes spike_op gradient from reset\n",
        "    mem = self.beta * mem + input_ - reset # Eq (1)\n",
        "    return spk, mem\n",
        "\n",
        "  # Forward pass: Heaviside function\n",
        "  # Backward pass: Override Dirac Delta with the Spike itself\n",
        "  @staticmethod\n",
        "  class SpikeOperator(torch.autograd.Function):\n",
        "      @staticmethod\n",
        "      def forward(ctx, mem):\n",
        "          spk = (mem > 0).float() # Heaviside on the forward pass: Eq(2)\n",
        "          ctx.save_for_backward(spk)  # store the spike for use in the backward pass\n",
        "          return spk\n",
        "\n",
        "      @staticmethod\n",
        "      def backward(ctx, grad_output):\n",
        "          (spk,) = ctx.saved_tensors  # retrieve the spike \n",
        "          grad = grad_output * spk # scale the gradient by the spike: 1/0\n",
        "          return grad"
      ],
      "metadata": {
        "id": "upjF3m1TUu4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce the above neuron using PyTorch\n",
        "\n",
        "lif1 = LeakySurrogate(beta = 0.9)\n",
        "lif1 = snn.Leaky(beta = 0.9) #the snn function applies the Spike Operator surrogate gradient by default"
      ],
      "metadata": {
        "id": "ORxm9x-gUv5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SETTING UP MNIST"
      ],
      "metadata": {
        "id": "7WarZkZsMBJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dataloader arguments\n",
        "batch_size = 128\n",
        "data_path='/data/mnist'\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "bkSiVj3LLsJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a transform\n",
        "transform = transforms.Compose([\n",
        "            transforms.Resize((28, 28)),\n",
        "            transforms.Grayscale(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0,), (1,))])\n",
        "\n",
        "mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n",
        "mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)"
      ],
      "metadata": {
        "id": "qX2GQ2dXL4bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoaders\n",
        "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=True)"
      ],
      "metadata": {
        "id": "SrHUyhIVL_MV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DEFINE THE NETWORK"
      ],
      "metadata": {
        "id": "sADPoq8dN--d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Network Architecture\n",
        "num_inputs = 28*28 #We transformed the MNIST dataset to dimensions (28,28) in above code\n",
        "num_hidden = 1000\n",
        "num_outputs = 10\n",
        "\n",
        "# Temporal Dynamics\n",
        "num_steps = 25\n",
        "beta = 0.95"
      ],
      "metadata": {
        "id": "6sgCYIQwOAoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Network\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Initialize layers\n",
        "        self.fc1 = nn.Linear(num_inputs, num_hidden) #Applies a linear transformation of all pixels from MNIST (inputs)\n",
        "        self.lif1 = snn.Leaky(beta=beta) #First spiking neuron layer: integrates weighted input over time and emits a spike if threshold is met\n",
        "        self.fc2 = nn.Linear(num_hidden, num_outputs) #Applies a linear transformation to the output spikes of fc1\n",
        "        self.lif2 = snn.Leaky(beta=beta) #Second spiking neuron layer: integrates weighted spikes over time\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Initialize hidden states at t=0\n",
        "        mem1 = self.lif1.init_leaky()\n",
        "        mem2 = self.lif2.init_leaky()\n",
        "        \n",
        "        # Record the final layer\n",
        "        spk2_rec = []\n",
        "        mem2_rec = []\n",
        "\n",
        "        for step in range(num_steps):\n",
        "            cur1 = self.fc1(x)\n",
        "            spk1, mem1 = self.lif1(cur1, mem1)\n",
        "            cur2 = self.fc2(spk1)\n",
        "            spk2, mem2 = self.lif2(cur2, mem2)\n",
        "            spk2_rec.append(spk2)\n",
        "            mem2_rec.append(mem2)\n",
        "\n",
        "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)\n",
        "        \n",
        "# Load the network onto CUDA if available\n",
        "net = Net().to(device)"
      ],
      "metadata": {
        "id": "O7AROlkNOCFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAINING THE SNN"
      ],
      "metadata": {
        "id": "VNns5RmEbSII"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Accuracy metric: from a batch of data passed through the network, sum all the spikes from each neuron over time and compare the index of the highest spike count to that of the target. If they match, the network correctly predicted the target. This is a way of assessing the accuracy of the network."
      ],
      "metadata": {
        "id": "TxoeVHFTbTMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_batch_accuracy(data, targets, train=False):\n",
        "    output, _ = net(data.view(batch_size, -1))\n",
        "    _, idx = output.sum(dim=0).max(1)\n",
        "    acc = np.mean((targets == idx).detach().cpu().numpy())\n",
        "\n",
        "    if train:\n",
        "        print(f\"Train set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
        "    else:\n",
        "        print(f\"Test set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
        "\n",
        "def train_printer():\n",
        "    print(f\"Epoch {epoch}, Iteration {iter_counter}\")\n",
        "    print(f\"Train Set Loss: {loss_hist[counter]:.2f}\")\n",
        "    print(f\"Test Set Loss: {test_loss_hist[counter]:.2f}\")\n",
        "    print_batch_accuracy(data, targets, train=True)\n",
        "    print_batch_accuracy(test_data, test_targets, train=False)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "wU2gldNdbV2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Define the loss function"
      ],
      "metadata": {
        "id": "9RfxE_5fcSPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = nn.CrossEntropyLoss() #Single PyTorch function that takes the softmax of the output layer and generates a loss at the output \n",
        "#(see Pytorch documentation for possible parameters)"
      ],
      "metadata": {
        "id": "8WEoDZgdcVOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Define the optimizer (Adam for this example)"
      ],
      "metadata": {
        "id": "ynncaCrzctKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
        "\n",
        "#Can adjust the learning rate (lr), and the betas as well?"
      ],
      "metadata": {
        "id": "IXIWLJMFcv6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Single training iteration"
      ],
      "metadata": {
        "id": "I-Wb2nzcc1eK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Take the first batch of data and load into CUDA\n",
        "\n",
        "data, targets = next(iter(train_loader))\n",
        "data = data.to(device)\n",
        "targets = targets.to(device)\n",
        "\n",
        "#Flatten the input data into a vector of size 784 then pass it into the network\n",
        "\n",
        "spk_rec, mem_rec = net(data.view(batch_size, -1)) #Batch_size was defined earlier as 784, then transformed into a 28*28\n",
        "#The input is taken across 25 time steps, 128 data samples, and 10 output neurons which you can see through print(mem_rec.size())\n",
        "\n",
        "#Calculate the loss by:\n",
        "\n",
        "#Initializing the total loss value\n",
        "loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
        "\n",
        "#Summing loss at every step\n",
        "for step in range(num_steps):\n",
        "  loss_val += loss(mem_rec[step], targets)\n",
        "\n",
        "#Printing the loss\n",
        "print(f\"Training loss: {loss_val.item():.3f}\")\n"
      ],
      "metadata": {
        "id": "QqlPdsbedBVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. The above code was for one training iteration, but the real training loop is:"
      ],
      "metadata": {
        "id": "VQK_SU8XgOl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 1 #Modify if you want to train for >1 epoch\n",
        "loss_hist = []\n",
        "test_loss_hist = []\n",
        "counter = 0\n",
        "\n",
        "# Outer training loop\n",
        "for epoch in range(num_epochs):\n",
        "    iter_counter = 0\n",
        "    train_batch = iter(train_loader)\n",
        "\n",
        "    # Minibatch training loop\n",
        "    for data, targets in train_batch: #taking a batch of data and loading it into CUDA\n",
        "        data = data.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # forward pass\n",
        "        net.train()\n",
        "        spk_rec, mem_rec = net(data.view(batch_size, -1)) #flatten the input data into a vector and pass it into the network\n",
        "\n",
        "        # initialize the loss & sum over time\n",
        "        loss_val = torch.zeros((1), dtype=dtype, device=device) #Initialize the total loss value\n",
        "        for step in range(num_steps): #Sum loss at every step\n",
        "            loss_val += loss(mem_rec[step], targets)\n",
        "\n",
        "        # Gradient calculation + weight update\n",
        "        optimizer.zero_grad() #Clear previously stored gradients\n",
        "        loss_val.backward() #Calculate the gradients\n",
        "        optimizer.step() #Weight update\n",
        "\n",
        "        # Store loss history for future plotting\n",
        "        loss_hist.append(loss_val.item())\n",
        "\n",
        "        # Test set\n",
        "        with torch.no_grad():\n",
        "            net.eval()\n",
        "            test_data, test_targets = next(iter(test_loader))\n",
        "            test_data = test_data.to(device)\n",
        "            test_targets = test_targets.to(device)\n",
        "\n",
        "            # Test set forward pass\n",
        "            test_spk, test_mem = net(test_data.view(batch_size, -1))\n",
        "\n",
        "            # Test set loss\n",
        "            test_loss = torch.zeros((1), dtype=dtype, device=device)\n",
        "            for step in range(num_steps):\n",
        "                test_loss += loss(test_mem[step], test_targets)\n",
        "            test_loss_hist.append(test_loss.item())\n",
        "\n",
        "            # Print train/test loss/accuracy\n",
        "            if counter % 50 == 0:\n",
        "                train_printer()\n",
        "            counter += 1\n",
        "            iter_counter +=1"
      ],
      "metadata": {
        "id": "LDg0S81HgTvH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}