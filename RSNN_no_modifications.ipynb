{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrewsiyoon/spiking-seRNN/blob/main/RSNN_no_modifications.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sYnyweJ3z7X",
        "outputId": "5f0bf8d4-92b2-4ed4-ee63-d09911f925f5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/spamham/miniconda3/envs/spikingsernn-01/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "#Imports -----\n",
        "\n",
        "import torch, torch.nn as nn\n",
        "import snntorch as snn\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIiTfqFx3z7Z"
      },
      "outputs": [],
      "source": [
        "#Random seeds -----\n",
        "\n",
        "random.seed(211)\n",
        "torch.manual_seed(211)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2TtxKbO3z7Z",
        "outputId": "0a68e2c5-99d5-4129-b116-c9e373b862bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: wget: command not found\n",
            "tar: Error opening archive: Failed to open 'MNIST.tar.gz'\n"
          ]
        }
      ],
      "source": [
        "#MNIST -----\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "#Define variables\n",
        "batch_size = 128\n",
        "data_path = '/data/mnist'\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "#Define transformations\n",
        "transform = transforms.Compose([\n",
        "            transforms.Resize((28, 28)),\n",
        "            transforms.Grayscale(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0,), (1,))])\n",
        "\n",
        "#Download MNIST (without permissions)\n",
        "!wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
        "!tar -zxvf MNIST.tar.gz\n",
        "\n",
        "#Define training and test sets\n",
        "mnist_train = datasets.MNIST(root = './', train=True, download=True, transform=transform)\n",
        "mnist_test = datasets.MNIST(root = './', train=False, download=True, transform=transform)\n",
        "\n",
        "#Create DataLoaders\n",
        "trainloader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, drop_last = True) #drop_last to remove last (incongruent) batch\n",
        "testloader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEcROAiV3z7a",
        "outputId": "4883e38e-27a5-4cb8-d522-4ce138d10d87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "60000\n",
            "10000\n",
            "torch.Size([128, 1, 28, 28])\n",
            "torch.Size([128])\n",
            "torch.Size([128, 1, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "#Dimensional analysis -----\n",
        "\n",
        "print(len(mnist_train))\n",
        "print(len(mnist_test))\n",
        "\n",
        "examples = enumerate(trainloader)\n",
        "batch_idx, (example_data, example_targets) = next(examples)\n",
        "print(example_data.shape)\n",
        "print(example_targets.shape)\n",
        "\n",
        "event_tensor, target = next(iter(trainloader))\n",
        "print(event_tensor.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vB9kYHJN3z7a"
      },
      "outputs": [],
      "source": [
        "#Model architecture -----\n",
        "\n",
        "#Size parameters\n",
        "num_inputs = 28*28\n",
        "num_hidden = 1000\n",
        "num_outputs = 10\n",
        "\n",
        "#Network parameters\n",
        "beta = 0.95\n",
        "num_steps = 25\n",
        "\n",
        "#Model definition\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        #Initialize layers\n",
        "        self.fc1 = nn.Linear(num_inputs, num_hidden)\n",
        "        self.lif1 = snn.RLeaky(beta = beta, linear_features = num_hidden)\n",
        "        self.fc2 = nn.Linear(num_hidden, num_outputs)\n",
        "        self.lif2 = snn.Leaky(beta = beta)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        #Initialize parameters\n",
        "        spk1, mem1 = self.lif1.init_rleaky() #init_rleaky() creates a tuple [_SpikeTensor, _SpikeTensor], assigns each to mem1 and spk1\n",
        "        mem2 = self.lif2.init_leaky() #init_leaky() creates a tensor _SpikeTensor\n",
        "\n",
        "        #Record final layer\n",
        "        spk_rec = []\n",
        "        mem_rec = []\n",
        "\n",
        "        #Forward loop\n",
        "        for step in range(num_steps):\n",
        "            cur1 = self.fc1(x)\n",
        "            spk1, mem1 = self.lif1(cur1, spk1, mem1)\n",
        "            cur2 = self.fc2(spk1)\n",
        "            spk2, mem2 = self.lif2(cur2, mem2)\n",
        "\n",
        "            spk_rec.append(spk2)\n",
        "            mem_rec.append(mem2)\n",
        "\n",
        "        #Convert final lists to tensors\n",
        "        spk_rec = torch.stack(spk_rec)\n",
        "        mem_rec = torch.stack(mem_rec)\n",
        "        \n",
        "        return spk_rec, mem_rec\n",
        "\n",
        "net = Net()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-pPT01O3z7b",
        "outputId": "ced2e04d-73a0-4baa-9abb-8baa9016fd65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer: fc1.weight | Size: torch.Size([1000, 784]) | Values : tensor([[-0.0191, -0.0323, -0.0147,  ...,  0.0102, -0.0193,  0.0085],\n",
            "        [ 0.0055, -0.0354,  0.0019,  ..., -0.0184, -0.0155,  0.0227]],\n",
            "       grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: fc1.bias | Size: torch.Size([1000]) | Values : tensor([-0.0323,  0.0102], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: lif1.recurrent.weight | Size: torch.Size([1000, 1000]) | Values : tensor([[-0.0070, -0.0091, -0.0068,  ..., -0.0155,  0.0268,  0.0038],\n",
            "        [ 0.0153,  0.0143,  0.0237,  ...,  0.0118,  0.0283,  0.0158]],\n",
            "       grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: lif1.recurrent.bias | Size: torch.Size([1000]) | Values : tensor([-0.0301,  0.0288], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: fc2.weight | Size: torch.Size([10, 1000]) | Values : tensor([[ 0.0164, -0.0059, -0.0019,  ..., -0.0150,  0.0082,  0.0209],\n",
            "        [-0.0230,  0.0164, -0.0230,  ..., -0.0090,  0.0147,  0.0184]],\n",
            "       grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: fc2.bias | Size: torch.Size([10]) | Values : tensor([0.0210, 0.0175], grad_fn=<SliceBackward0>) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Model visualizations -----\n",
        "\n",
        "#All layers and associated parameters\n",
        "for name, param in net.named_parameters():\n",
        "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")\n",
        "\n",
        "#Dataset dimensions\n",
        "\n",
        "\n",
        "#Visual map\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fNVAxQP3z7b"
      },
      "outputs": [],
      "source": [
        "#Optimizer and loss function\n",
        "\n",
        "import snntorch.functional as SF\n",
        "\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr = 2e-3, betas = (0.9, 0.999))\n",
        "loss_fn = SF.mse_count_loss(correct_rate = 0.8, incorrect_rate = 0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3iF1Kjm3z7b",
        "outputId": "99a2032a-6ef5-4df3-ec0b-63000621b9b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Iteration 0 \n",
            "Train Loss: 2.41\n",
            "Accuracy: 7.03%\n",
            "\n",
            "Epoch 0, Iteration 25 \n",
            "Train Loss: 0.57\n",
            "Accuracy: 57.81%\n",
            "\n",
            "Epoch 0, Iteration 50 \n",
            "Train Loss: 0.32\n",
            "Accuracy: 81.25%\n",
            "\n",
            "Epoch 0, Iteration 75 \n",
            "Train Loss: 0.21\n",
            "Accuracy: 89.06%\n",
            "\n",
            "Epoch 0, Iteration 100 \n",
            "Train Loss: 0.15\n",
            "Accuracy: 92.97%\n",
            "\n",
            "Epoch 0, Iteration 125 \n",
            "Train Loss: 0.19\n",
            "Accuracy: 92.19%\n",
            "\n",
            "Epoch 0, Iteration 150 \n",
            "Train Loss: 0.19\n",
            "Accuracy: 86.72%\n",
            "\n",
            "Epoch 0, Iteration 175 \n",
            "Train Loss: 0.14\n",
            "Accuracy: 94.53%\n",
            "\n",
            "Epoch 0, Iteration 200 \n",
            "Train Loss: 0.13\n",
            "Accuracy: 95.31%\n",
            "\n",
            "Epoch 0, Iteration 225 \n",
            "Train Loss: 0.17\n",
            "Accuracy: 88.28%\n",
            "\n",
            "Epoch 0, Iteration 250 \n",
            "Train Loss: 0.17\n",
            "Accuracy: 86.72%\n",
            "\n",
            "Epoch 0, Iteration 275 \n",
            "Train Loss: 0.11\n",
            "Accuracy: 94.53%\n",
            "\n",
            "Epoch 0, Iteration 300 \n",
            "Train Loss: 0.10\n",
            "Accuracy: 96.09%\n",
            "\n",
            "Epoch 0, Iteration 325 \n",
            "Train Loss: 0.09\n",
            "Accuracy: 95.31%\n",
            "\n",
            "Epoch 0, Iteration 350 \n",
            "Train Loss: 0.11\n",
            "Accuracy: 94.53%\n",
            "\n",
            "Epoch 0, Iteration 375 \n",
            "Train Loss: 0.11\n",
            "Accuracy: 95.31%\n",
            "\n",
            "Epoch 0, Iteration 400 \n",
            "Train Loss: 0.11\n",
            "Accuracy: 93.75%\n",
            "\n",
            "Epoch 0, Iteration 425 \n",
            "Train Loss: 0.11\n",
            "Accuracy: 92.97%\n",
            "\n",
            "Epoch 0, Iteration 450 \n",
            "Train Loss: 0.15\n",
            "Accuracy: 91.41%\n",
            "\n",
            "Epoch 1, Iteration 0 \n",
            "Train Loss: 0.12\n",
            "Accuracy: 94.53%\n",
            "\n",
            "Epoch 1, Iteration 25 \n",
            "Train Loss: 0.10\n",
            "Accuracy: 96.88%\n",
            "\n",
            "Epoch 1, Iteration 50 \n",
            "Train Loss: 0.14\n",
            "Accuracy: 92.19%\n",
            "\n",
            "Epoch 1, Iteration 75 \n",
            "Train Loss: 0.15\n",
            "Accuracy: 91.41%\n",
            "\n",
            "Epoch 1, Iteration 100 \n",
            "Train Loss: 0.12\n",
            "Accuracy: 91.41%\n",
            "\n",
            "Epoch 1, Iteration 125 \n",
            "Train Loss: 0.09\n",
            "Accuracy: 96.09%\n",
            "\n",
            "Epoch 1, Iteration 150 \n",
            "Train Loss: 0.12\n",
            "Accuracy: 92.97%\n",
            "\n",
            "Epoch 1, Iteration 175 \n",
            "Train Loss: 0.12\n",
            "Accuracy: 92.97%\n",
            "\n",
            "Epoch 1, Iteration 200 \n",
            "Train Loss: 0.10\n",
            "Accuracy: 96.09%\n",
            "\n",
            "Epoch 1, Iteration 225 \n",
            "Train Loss: 0.11\n",
            "Accuracy: 93.75%\n",
            "\n",
            "Epoch 1, Iteration 250 \n",
            "Train Loss: 0.12\n",
            "Accuracy: 92.97%\n",
            "\n",
            "Epoch 1, Iteration 275 \n",
            "Train Loss: 0.09\n",
            "Accuracy: 94.53%\n",
            "\n",
            "Epoch 1, Iteration 300 \n",
            "Train Loss: 0.08\n",
            "Accuracy: 95.31%\n",
            "\n",
            "Epoch 1, Iteration 325 \n",
            "Train Loss: 0.08\n",
            "Accuracy: 96.88%\n",
            "\n",
            "Epoch 1, Iteration 350 \n",
            "Train Loss: 0.11\n",
            "Accuracy: 92.97%\n",
            "\n",
            "Epoch 1, Iteration 375 \n",
            "Train Loss: 0.09\n",
            "Accuracy: 95.31%\n",
            "\n",
            "Epoch 1, Iteration 400 \n",
            "Train Loss: 0.08\n",
            "Accuracy: 94.53%\n",
            "\n",
            "Epoch 1, Iteration 425 \n",
            "Train Loss: 0.11\n",
            "Accuracy: 92.97%\n",
            "\n",
            "Epoch 1, Iteration 450 \n",
            "Train Loss: 0.09\n",
            "Accuracy: 95.31%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Training paradigm -----\n",
        "\n",
        "#Training parameters\n",
        "num_epochs = 2\n",
        "num_steps = 25\n",
        "counter = 0\n",
        "\n",
        "#Initialize loss and accuracy \n",
        "loss_hist = []\n",
        "acc_hist = []\n",
        "\n",
        "#Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (data, targets) in enumerate(iter(trainloader)):\n",
        "\n",
        "        #Load on CUDA (if available)\n",
        "        data = data.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        #Set model to training mode\n",
        "        net.train()\n",
        "        outputs, _ = net(data.view(batch_size, -1))\n",
        "\n",
        "        #Calculate loss\n",
        "        loss_val = loss_fn(outputs, targets)\n",
        "\n",
        "        #Gradient calculation and weight updates\n",
        "        optimizer.zero_grad()\n",
        "        loss_val.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        #Store loss history\n",
        "        loss_hist.append(loss_val.item())\n",
        "\n",
        "        #Prints (print every 25 iterations)\n",
        "        if i % 25 == 0:\n",
        "            net.eval()\n",
        "\n",
        "            #Print training loss\n",
        "            print(f\"Epoch {epoch}, Iteration {i} \\nTrain Loss: {loss_val.item():.2f}\")\n",
        "\n",
        "            #Model performance on single batch\n",
        "            acc = SF.accuracy_rate(outputs, targets) #Outputs: [num_steps, batch_size, num_outputs]. Targets: [batch_size]\n",
        "            acc_hist.append(acc)\n",
        "            print(f\"Accuracy: {acc * 100:.2f}%\\n\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "spikingsernn-01",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "bf4fc4f4f5edd90960871ab78eee9b06884170e3a238fe1d4a227e78e9010765"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}